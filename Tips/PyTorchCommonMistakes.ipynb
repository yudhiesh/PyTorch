{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchCommonMistakes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSp_n3JHYH2w"
      },
      "source": [
        "# PyTorch Common Mistakes and ways to mitigate them\n",
        "\n",
        "This is just a guide on things that you can do to ensure you do not waste time when you are training neural networks\n",
        "\n",
        "## 1. You did not overfit a single batch at first\n",
        "\n",
        "The logic here is simple if you cannot even overfit a single batch then you will not be able to train on the full dataset properly\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "// this will only iterate over a single example from the train loader\n",
        "\n",
        "data, targets = next(iter(train_loader))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  // comment out the line below \n",
        "  \n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFkSdfIVYpij"
      },
      "source": [
        "## 2. You forgot to set training or evaluation mode\n",
        "\n",
        "When checking the models accuracies make sure that you are first toggling the *eval()* mode before doing so. \n",
        "\n",
        "When eval() is called the model does not use dropout or batchnormalization and will produce better accuracies compared to if you were not to use it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QDpgKa1YpJk"
      },
      "source": [
        "## 3. You forgot to use .zero_grad()\n",
        "\n",
        "*optimizer.zero_grad()* before the *loss.backward()* if you do not zero the gradients then you are basically using the gradients for all the previous batches that are accumulated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQE8k3auYoqh"
      },
      "source": [
        "## 4. Using Softmax with CrossEntropy\n",
        "\n",
        "When using Softmax as the output layer and then CrossEntropy loss which combines *nn.LogSoftmax()* and *nn.NLLLoss()* in one single class.\n",
        "\n",
        "This will result in a slight drop in accuracy but it is still something that should be prevented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "297-uL6LYoax"
      },
      "source": [
        "## 5. Using bias when using BatchNorm\n",
        "\n",
        "You have to set the **bias=False** because the bias is unnecessary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxBJJKgQYoRa"
      },
      "source": [
        "## 6. Using view() as permute\n",
        "\n",
        "Using view() will not have the same affects as using permute() as view() just simply places the elements in any order to satisfy the shape that is specified. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7KEZ3aPYoKK"
      },
      "source": [
        "## 7. Using bad data augmentation \n",
        "\n",
        "For example do not modify the target output such as using RandomFlipVertical and RandomFlipHorizontal with a probability of 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YH6RHzrYoET"
      },
      "source": [
        "## 8. Not shuffling the data \n",
        "\n",
        "*When using time-series data you would not want to shuffle the data as the order matters when training the model*\n",
        "\n",
        "For other things you can then just set *shuffle=True* in the DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-lrCVFAYn-5"
      },
      "source": [
        "## 9. Not Normalizing the data\n",
        "\n",
        "When setting transforms.Compose() you need to include *transforms.Normalize* with the accurate mean and standard deviation based on the data you have. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15gHCREtYn40"
      },
      "source": [
        "## 10. Not clippling the gradients(when using RNNs, LSTMs, GRUs)\n",
        "\n",
        "Without it the model would suffer from the exploding gradient problem.\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "torn.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n",
        "\n"
      ]
    }
  ]
}